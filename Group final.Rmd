---
title: "Fin 550 Final Project"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(dplyr)
library(writexl)
library(readr)
library(glmnet)
```

```{r}
historic_property_data <- read_csv("historic_property_data.csv")

predict_property_data <- read_csv("predict_property_data.csv")
historic_property_data$meta_town_code <- as.character(historic_property_data$meta_town_code)
predict_property_data$meta_town_code <- as.character(predict_property_data$meta_town_code)

```

```{r}
# historic property data
# Non-predictor variables to be removed
# non_predictor_vars <- c('char_cnst_qlty', 'char_ot_impr', 'char_renovation', 
#                         'char_repair_cnd', 'char_site', 'geo_asian_perc',
#                         'geo_black_perc', 'geo_fips', 'geo_his_perc',
#                         'geo_municipality', 'geo_other_perc', 'geo_property_city',
#                         'geo_property_zip', 'geo_tract_pop', 'geo_white_perc',
#                         'ind_large_home', 'meta_cdu', 'meta_certified_est_bldg',
#                         'meta_certified_est_land', 'meta_class', 'meta_deed_type','meta_nbhd')

non_predictor_vars <- c('char_cnst_qlty', 'char_ot_impr', 'char_renovation',
'char_repair_cnd', 'char_site', 'geo_fips',
'geo_municipality', 'geo_property_city',
'geo_property_zip', 'geo_tract_pop',
'ind_large_home', 'meta_cdu', 'meta_class', 'meta_deed_type','meta_nbhd')

# Remove the non-predictor columns from the historic_property_data
historic_property_data_clean <- historic_property_data %>%
  select(-all_of(non_predictor_vars))

# View the cleaned dataframe
head(historic_property_data_clean)

# export cleaned data to xlsx
#write_xlsx(historic_property_data_clean, "historic_property_data_clean.xlsx")


```

```{r}

# Remove the non-predictor columns from the predict_property_data
predict_property_data_clean <- predict_property_data %>%
  select(-all_of(non_predictor_vars))

# View the cleaned dataframe
head(predict_property_data_clean)

#write_xlsx(predict_property_data_clean, "predict_property_data_clean.xlsx")
```
```{r}
# Removing Columns with more than 50% missing values
# Specify the columns to remove
columns_to_remove <- c("char_apts", "char_tp_dsgn", "char_attic_fnsh", "char_porch")

# Remove the columns from historic_property_data_clean
historic_property_data_clean <- historic_property_data_clean[ , !(names(historic_property_data_clean) %in% columns_to_remove)]


# Remove the same columns from predict_property_data_clean
predict_property_data_clean <- predict_property_data_clean[ , !(names(predict_property_data_clean) %in% columns_to_remove)]


```



```{r}
# Function to calculate mode
get_mode <- function(v) {
  uniqv <- unique(na.omit(v))
  uniqv[which.max(tabulate(match(v, uniqv)))]
}

# Define numerical and categorical columns
numerical_columns <- c("char_hd_sf", "char_age", "char_bldg_sf", "econ_tax_rate", "econ_midincome")
numerical_columns_historic <- c(numerical_columns, "sale_price")
categorical_columns <- setdiff(names(historic_property_data_clean), numerical_columns_historic)

# Replace missing values in categorical columns with mode
historic_property_data_clean[categorical_columns] <- lapply(historic_property_data_clean[categorical_columns], function(x) ifelse(is.na(x), get_mode(x), x))
predict_property_data_clean[categorical_columns] <- lapply(predict_property_data_clean[categorical_columns], function(x) ifelse(is.na(x), get_mode(x), x))

# Replace missing values in numerical columns with mean
historic_property_data_clean[numerical_columns_historic] <- lapply(historic_property_data_clean[numerical_columns_historic], function(x) ifelse(is.na(x), mean(x, na.rm = TRUE), x))
predict_property_data_clean[numerical_columns] <- lapply(predict_property_data_clean[numerical_columns], function(x) ifelse(is.na(x), mean(x, na.rm = TRUE), x))

# Check if all missing values are filled
sum(is.na(historic_property_data_clean))
sum(is.na(predict_property_data_clean))

str(historic_property_data_clean)

```
```{r}
## Winsorize Variables and Remopve Negative Values
# Function to cap and floor values based on quantiles
cap_floor_values <- function(data, column) {
  lower_bound <- quantile(data[[column]], 0.01, na.rm = TRUE)
  upper_bound <- quantile(data[[column]], 0.99, na.rm = TRUE)
  
  data[[column]] <- ifelse(data[[column]] < lower_bound, lower_bound, data[[column]])
  data[[column]] <- ifelse(data[[column]] > upper_bound, upper_bound, data[[column]])
  
  return(data)
}

# Apply capping and flooring for each numerical column in historic_property_data_clean
for (col in numerical_columns_historic) {
  historic_property_data_clean <- cap_floor_values(historic_property_data_clean, col)
}

# Apply capping and flooring for each numerical column in predict_property_data_clean
for (col in numerical_columns) {
  predict_property_data_clean <- cap_floor_values(predict_property_data_clean, col)
}

# Ensuring that predicted_price (if exists in your datasets) is non-negative
# Assuming 'predicted_price' is a column you plan to create or have in your data
# Uncomment and adjust the following line if applicable
historic_property_data_clean <- historic_property_data_clean[historic_property_data_clean$sale_price >= 0, ]
# predict_property_data_clean <- predict_property_data_clean[predict_property_data_clean$predicted_price >= 0, ]

```

```{r}
unique_values_count <- sapply(historic_property_data_clean, function(x) length(unique(x)))
unique_values_count
# Merge columns related to location
#historic_property_data_clean <- historic_property_data_clean %>%
  #mutate(location_combined = paste(meta_town_code, meta_nbhd, sep = "_")) %>%
  #select(-meta_town_code, -meta_nbhd) # Remove the original columns after merging

# Merge columns related to basement characteristics
historic_property_data_clean <- historic_property_data_clean %>%
  mutate(basement_combined = paste(char_bsmt, char_bsmt_fin, sep = "_")) %>%
  select(-char_bsmt, -char_bsmt_fin) # Remove the original columns after merging

# Merge columns related to heating and cooling systems
historic_property_data_clean <- historic_property_data_clean %>%
  mutate(climate_control = paste(char_heat, char_oheat, char_air, sep = "_")) %>%
  select(-char_heat, -char_oheat, -char_air) # Remove the original columns after merging

# Merge columns related to garage
#historic_property_data_clean <- historic_property_data_clean %>%
  #mutate(garage_combined = paste(char_gar1_size, char_gar1_cnst, char_gar1_att, char_gar1_area, sep = "_")) %>%
  #select(-char_gar1_size, -char_gar1_cnst, -char_gar1_att, -char_gar1_area) # Remove the original columns after merging

# Merge columns related to geographical risk factors
#historic_property_data_clean <- historic_property_data_clean %>%
  #mutate(geographical_risk = paste(geo_ohare_noise, geo_floodplain, geo_fs_flood_factor, geo_fs_flood_risk_direction, sep = "_")) %>%
  #select(-geo_ohare_noise, -geo_floodplain, -geo_fs_flood_factor, -geo_fs_flood_risk_direction) # Remove the original columns after merging

# The resulting dataframe
print(historic_property_data_clean)
```


```{r}

# Merging Columns in Predicted Dataset
# Merge columns related to location
#predict_property_data_clean <- predict_property_data_clean %>%
  #mutate(location_combined = paste(meta_town_code, meta_nbhd, sep = "_")) %>%
  #select(-meta_town_code, -meta_nbhd) # Remove the original columns after merging

# Merge columns related to basement characteristics
predict_property_data_clean <- predict_property_data_clean %>%
  mutate(basement_combined = paste(char_bsmt, char_bsmt_fin, sep = "_")) %>%
  select(-char_bsmt, -char_bsmt_fin) # Remove the original columns after merging

# Merge columns related to heating and cooling systems
predict_property_data_clean <- predict_property_data_clean %>%
  mutate(climate_control = paste(char_heat, char_oheat, char_air, sep = "_")) %>%
  select(-char_heat, -char_oheat, -char_air) # Remove the original columns after merging

# Merge columns related to garage
#predict_property_data_clean <- predict_property_data_clean %>%
  #mutate(garage_combined = paste(char_gar1_size, char_gar1_cnst, char_gar1_att, char_gar1_area, sep = "_")) %>%
  #select(-char_gar1_size, -char_gar1_cnst, -char_gar1_att, -char_gar1_area) # Remove the original columns after merging

# Merge columns related to geographical risk factors
#predict_property_data_clean <- predict_property_data_clean %>%
  #mutate(geographical_risk = paste(geo_ohare_noise, geo_floodplain, geo_fs_flood_factor, geo_fs_flood_risk_direction, sep = "_")) %>%
  #select(-geo_ohare_noise, -geo_floodplain, -geo_fs_flood_factor, -geo_fs_flood_risk_direction) # Remove the original columns after merging

# The resulting dataframe
print(predict_property_data_clean)
```


```{r}
write_xlsx(historic_property_data_clean, "historic_property_data_clean.xlsx")
write_xlsx(predict_property_data_clean, "predict_property_data_clean.xlsx")
```

```{r}
str(historic_property_data_clean)
unique_values_count <- sapply(historic_property_data_clean, function(x) length(unique(x)))
unique_values_count
```

```{r}
# Check the correlation among variables and prevent the problem of multicollinearity for historic_property_data_clean dataset

colnames(historic_property_data_clean)

str(historic_property_data_clean)

#historic_property_data_clean1 <- historic_property_data_clean %>% select(-c("geo_school_elem_district","geo_school_hs_district","meta_town_code","basement_combined","climate_control","garage_combined","geographical_risk"))

historic_property_data_clean1 <- historic_property_data_clean %>% select(-c("geo_school_elem_district","geo_school_hs_district","meta_town_code","basement_combined","climate_control","char_gar1_size", "char_gar1_cnst", "char_gar1_att", "char_gar1_area","geo_ohare_noise", "geo_floodplain", "geo_fs_flood_factor", "geo_fs_flood_risk_direction"))

#str(historic_property_data_clean1)
# Create a correlation matrix
correlation_matrix <- cor(historic_property_data_clean1)

# Set the threshold for correlation
threshold <- 0.75

# Find highly correlated variables
highly_correlated <- which(correlation_matrix > threshold & correlation_matrix < 1.0, arr.ind = TRUE)

# Display the highly correlated pairs
 for (i in 1:nrow(highly_correlated)) {
   row_index <- highly_correlated[i, 1]
   col_index <- highly_correlated[i, 2]
   variable1 <- colnames(correlation_matrix)[row_index]
   variable2 <- colnames(correlation_matrix)[col_index]
   cor_value <- correlation_matrix[row_index, col_index]
   
   cat("Correlation between", variable1, "and", variable2, ":", cor_value, "\n")
 }

# Manually drop one variable from each highly correlated pair based on your decision
variables_to_drop <- c("char_beds","char_fbath")

# Drop the selected variables
historic_property_data_clean2 <- historic_property_data_clean1[, !(colnames(historic_property_data_clean1) %in% variables_to_drop)]

# Manually add back categorical variables
#variables_to_add <- c("geo_school_elem_district","geo_school_hs_district","meta_town_code","basement_combined","climate_control","garage_combined","geographical_risk")


variables_to_add <- c("geo_school_elem_district","geo_school_hs_district","meta_town_code","basement_combined","climate_control","char_gar1_size", "char_gar1_cnst", "char_gar1_att", "char_gar1_area","geo_ohare_noise", "geo_floodplain", "geo_fs_flood_factor", "geo_fs_flood_risk_direction")

# Add the selected variables
#historic_property_data_clean2 <- cbind(historic_property_data_clean2, historic_property_data_clean[, variables_to_add])

# Add and convert the selected variables
historic_property_data_clean2 <- cbind(historic_property_data_clean2, lapply(historic_property_data_clean[, variables_to_add], as.character))

str(historic_property_data_clean2)

# Now, historic_property_data_clean2 contains variables with reduced multicollinearity
```

```{r}

predict_property_data_clean1 <- predict_property_data_clean %>%    select(-c("geo_school_elem_district","geo_school_hs_district","meta_town_code","basement_combined","climate_control","char_gar1_size", "char_gar1_cnst", "char_gar1_att", "char_gar1_area","geo_ohare_noise", "geo_floodplain", "geo_fs_flood_factor", "geo_fs_flood_risk_direction"))

# Manually drop one variable from each highly correlated pair based on your decision
variables_to_drop <- c("char_beds","char_fbath")

# Drop the selected variables
predict_property_data_clean2 <- predict_property_data_clean1[, !(colnames(predict_property_data_clean1) %in% variables_to_drop)]


variables_to_add <- c("geo_school_elem_district","geo_school_hs_district","meta_town_code","basement_combined","climate_control","char_gar1_size", "char_gar1_cnst", "char_gar1_att", "char_gar1_area","geo_ohare_noise", "geo_floodplain", "geo_fs_flood_factor", "geo_fs_flood_risk_direction")


# Add and convert the selected variables
predict_property_data_clean2 <- cbind(predict_property_data_clean2, lapply(predict_property_data_clean[, variables_to_add], as.character))



```


```{r}
##Data Partition
# set seed for reproducing the partition 
set.seed(1) 

# row numbers of the training set
train.index <- sample(c(1:dim(historic_property_data_clean2)[1]), dim(historic_property_data_clean2)[1]*0.6)  
head(train.index)

# training set 
train.df <- historic_property_data_clean2[train.index, ]
head(train.df)

# test set 
test.df <- historic_property_data_clean2[-train.index, ]
head(test.df)
```
```{r}
# convert a data frame of predictors to a matrix and create dummy variables for character variables 
x <- model.matrix(sale_price~., historic_property_data_clean2)[,-1]

# outcome 
y <- historic_property_data_clean2$sale_price

test.index <- setdiff(c(1:dim(x)[1]), train.index)
y.test <- y[test.index]
```


```{r}
# fit a lasso regression model 
fit <- glmnet(x[train.index,], y[train.index], alpha = 1)
# sequence of lambda values 
fit$lambda
# dimension of lasso regression coefficients 
dim(coef(fit))
# plot coefficients on log of lambda values 
plot(fit, xvar = "lambda")

```
```{r}
# set seed 
set.seed(1)
# 5-fold cross validation 
cv.fit <- cv.glmnet(x[train.index,], y[train.index], alpha = 1, nfolds = 5, type.measure = "mse")
# plot the cross-validated MSE for each lambda 
plot(cv.fit)
# lambda that corresponds to the lowest cross-validated MSE 
lambda.best <- cv.fit$lambda.min
lambda.best
```



```{r}
# lasso regression coefficients  
coef.lambda.best <- predict(fit, s=lambda.best, type = "coefficients")

# make predictions for records in the test set 
pred.lambda.best <- predict(fit, s=lambda.best, newx = x[test.index,])
# MSE in the test set (lowest MSE)
mean((y.test - pred.lambda.best)^2)

```
##The following part is forward selection/backward selection, but they will take extremely long time to execute because of the number of dummy virables. If you need to run, please uncomment the following code.

```{r}
# Perform backward elimination
# fit the model with all predictors  
# lm.full <- lm(sale_price ~ ., data = train.df)
#lm.full

# use step() to run backward elimination 
#lm.step.backward <- step(lm.full, direction = "backward")

# summary table 
#summary(lm.step.backward)  

# making predictions on the test set
#lm.step.pred.backward <- predict(lm.step.backward, test.df)
#head(lm.step.pred.backward)

# MSE in the test set 
#mean((test.df$sale_price-lm.step.pred.backward)^2)

```

```{r}
# Perform forward selection

# create model with no predictors
#lm.null <- lm(sale_price~1, data = train.df)
#lm.null

# use step() to run forward selection  
#lm.step.forward <- step(lm.null, scope=list(lower=lm.null, upper=lm.full), direction = "forward")

# summary table
#summary(lm.step.forward)

# making predictions on the test set
#lm.step.pred.forward <- predict(lm.step.forward, test.df)
#head(lm.step.pred.forward)

# MSE in the test set 
#mean((test.df$sale_price-lm.step.pred.forward)^2)
```



```{r}
# Extract coefficients for the optimal lambda
selected_features <- coef(fit, s = lambda.best)
 
# Identify selected variables with non-zero coefficients
selected_variables <- rownames(selected_features)[selected_features[, 1] != 0]
 
# Display or use the selected variables
print(selected_variables)
```

```{r}
# List of columns to keep
columns_to_keep <- c(
  "sale_price", "meta_certified_est_bldg", "meta_certified_est_land", "char_hd_sf", "char_age",
  "char_ext_wall", "char_roof_cnst", "char_rooms", "char_frpl", "char_attic_type",
  "char_hbath", "char_tp_plan", "char_bldg_sf", "char_use", "char_type_resd",
  "geo_white_perc", "geo_black_perc", "geo_asian_perc", "geo_his_perc", "geo_other_perc",
  "geo_withinmr100", "geo_withinmr101300", "econ_tax_rate", "econ_midincome",
  "ind_garage", "ind_arms_length",
  "meta_town_code", "basement_combined", "climate_control", "char_gar1_size",
  "char_gar1_cnst", "char_gar1_att", "geo_floodplain", "geo_fs_flood_factor",
  "geo_fs_flood_risk_direction"
)
 
# Drop columns not in the list
train.df <- train.df[, names(train.df) %in% columns_to_keep]
dim(train.df)
```

```{r}
# Specify the columns to be converted to factors
columns_to_convert <- c(
  "ind_garage", "ind_arms_length", 
  "char_ext_wall", "char_roof_cnst", "char_attic_type","char_tp_plan", "char_use", "char_type_resd", 
  "geo_withinmr100", "geo_withinmr101300", 
  "meta_town_code", "basement_combined", 
  "climate_control", "char_gar1_size", 
  "char_gar1_cnst", "char_gar1_att", 
  "geo_floodplain", "geo_fs_flood_factor", 
  "geo_fs_flood_risk_direction"
)
 
# Convert specified columns to factors
train.df[columns_to_convert] <- lapply(train.df[columns_to_convert], as.factor)
str(train.df)
```


```{r}
# List of columns to keep
columns_to_keep <- c(
  "sale_price", "meta_certified_est_bldg", "meta_certified_est_land", "char_hd_sf", "char_age",
  "char_ext_wall", "char_roof_cnst", "char_rooms", "char_frpl", "char_attic_type",
  "char_hbath", "char_tp_plan", "char_bldg_sf", "char_use", "char_type_resd",
  "geo_white_perc", "geo_black_perc", "geo_asian_perc", "geo_his_perc", "geo_other_perc",
  "geo_withinmr100", "geo_withinmr101300", "econ_tax_rate", "econ_midincome",
  "ind_garage", "ind_arms_length",
  "meta_town_code", "basement_combined", "climate_control", "char_gar1_size",
  "char_gar1_cnst", "char_gar1_att", "geo_floodplain", "geo_fs_flood_factor",
  "geo_fs_flood_risk_direction"
)
 
# Drop columns not in the list
test.df <- test.df[, names(test.df) %in% columns_to_keep]
dim(test.df)
```

```{r}
# Specify the columns to be converted to factors
columns_to_convert <- c(
  "ind_garage", "ind_arms_length", 
  "char_ext_wall", "char_roof_cnst", "char_attic_type","char_tp_plan", "char_use", "char_type_resd", 
  "geo_withinmr100", "geo_withinmr101300", 
  "meta_town_code", "basement_combined", 
  "climate_control", "char_gar1_size", 
  "char_gar1_cnst", "char_gar1_att", 
  "geo_floodplain", "geo_fs_flood_factor", 
  "geo_fs_flood_risk_direction"
)
 
# Convert specified columns to factors
test.df[columns_to_convert] <- lapply(test.df[columns_to_convert], as.factor)
str(test.df)
```


```{r}
# Implement Random Forest algorithm

library(randomForest)

set.seed(1)
rf <- randomForest(sale_price ~ ., data = train.df, mtry = 4)
rf
```
```{r}
# Make predictions on the test data
predictions <- predict(rf, newdata = test.df)

# Calculate the Mean Squared Error
mse <- mean((test.df$sale_price - predictions)^2)

# Print the MSE
print(mse)
```
```{r}
# List of columns to keep
columns_to_keep <- c(
  "sale_price", "meta_certified_est_bldg", "meta_certified_est_land", "char_hd_sf", "char_age",
  "char_ext_wall", "char_roof_cnst", "char_rooms", "char_frpl", "char_attic_type",
  "char_hbath", "char_tp_plan", "char_bldg_sf", "char_use", "char_type_resd",
  "geo_white_perc", "geo_black_perc", "geo_asian_perc", "geo_his_perc", "geo_other_perc",
  "geo_withinmr100", "geo_withinmr101300", "econ_tax_rate", "econ_midincome",
  "ind_garage", "ind_arms_length",
  "meta_town_code", "basement_combined", "climate_control", "char_gar1_size",
  "char_gar1_cnst", "char_gar1_att", "geo_floodplain", "geo_fs_flood_factor",
  "geo_fs_flood_risk_direction"
)
 
# Drop columns not in the list
predict_property_data_final <- predict_property_data_clean2[, names(predict_property_data_clean2) %in% columns_to_keep]
dim(predict_property_data_final)
```

```{r}
# Specify the columns to be converted to factors
columns_to_convert <- c(
  "ind_garage", "ind_arms_length", 
  "char_ext_wall", "char_roof_cnst", "char_attic_type","char_tp_plan", "char_use", "char_type_resd", 
  "geo_withinmr100", "geo_withinmr101300", 
  "meta_town_code", "basement_combined", 
  "climate_control", "char_gar1_size", 
  "char_gar1_cnst", "char_gar1_att", 
  "geo_floodplain", "geo_fs_flood_factor", 
  "geo_fs_flood_risk_direction"
)
 
# Convert specified columns to factors
predict_property_data_final[columns_to_convert] <- lapply(predict_property_data_final[columns_to_convert], as.factor)
str(predict_property_data_final)
```


```{r}
# variable importance 
importance(rf)

# variable importance plot
varImpPlot(rf)
```

```{r}
# Adding a column 'sale_price' at the beginning with placeholder numeric values
predict_property_data_final <- data.frame(sale_price = rep(0, nrow(predict_property_data_final)), predict_property_data_final)

```


```{r}
# Align the columns of prediction data with training data (excluding 'sale_price')
aligned_predict_data <- predict_property_data_final[colnames(train.df)[-which(colnames(train.df) == "sale_price")]]

# Now, bind the first row of the training set (excluding 'sale_price') and prediction set
temp_data <- rbind(train.df[1, -which(names(train.df) == "sale_price")], aligned_predict_data)

# Remove the first row (which came from the training set)
temp_data <- temp_data[-1, ]
# Make predictions with the aligned data
predicted_sale_prices <- predict(rf, newdata = temp_data)

```

```{r}
# Creating the 'pid' column
predict_property_data_final$pid <- 1:nrow(predict_property_data_final)

results_df <- data.frame(pid = predict_property_data_final$pid, 
                         assessed_value = predicted_sale_prices)

# Write to CSV file
write.csv(results_df, "assessed_value.csv", row.names = FALSE)

```


